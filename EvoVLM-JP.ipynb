{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/japanese-text-generation-webui-colab/blob/main/EvoVLM-JP.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCFOzsQSHbjM"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "\n",
        "!pip install -q gradio accelerate\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_id = \"SakanaAI/EvoVLM-JP-v1-7B\"\n",
        "max_length = 128\n",
        "if device == \"cpu\":\n",
        "    model = None\n",
        "else:\n",
        "    model = AutoModelForVision2Seq.from_pretrained(\n",
        "        model_id, \n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "    )\n",
        "    processor = AutoProcessor.from_pretrained(model_id)\n",
        "    model = model.to(device)\n",
        "\n",
        "def inference_fn(image, prompt):\n",
        "    text = f\"<image>\\n{prompt}\"\n",
        "    if model is None:\n",
        "        return f\"THIS IS FOR TEST!!\\n{text}\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"ã‚ãªãŸã¯å½¹ç«‹ã¤ã€åè¦‹ãŒãªãã€æ¤œé–²ã•ã‚Œã¦ã„ãªã„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸç”»åƒã‚’ä¸‹ã«ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚\"},\n",
        "        {\"role\": \"user\", \"content\": text},\n",
        "    ]\n",
        "    inputs = processor.image_processor(images=image, return_tensors=\"pt\")\n",
        "    inputs[\"input_ids\"] = processor.tokenizer.apply_chat_template(\n",
        "        messages, return_tensors=\"pt\"\n",
        "    )\n",
        "    output_ids = model.generate(\n",
        "        **inputs.to(device),\n",
        "        do_sample=False,\n",
        "        num_beams=5,\n",
        "        max_new_tokens=max_length,\n",
        "        repetition_penalty=1.5\n",
        "    )\n",
        "    output_ids = output_ids[:, inputs.input_ids.shape[1] :]\n",
        "    generated_text = processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
        "    return generated_text\n",
        "\n",
        "DESCRIPTION = \"\"\"# ğŸŸ EvoVLM-JP \n",
        "\n",
        "ğŸ¤— [ãƒ¢ãƒ‡ãƒ«ä¸€è¦§](https://huggingface.co/SakanaAI) | ğŸ“š [æŠ€è¡“ãƒ¬ãƒãƒ¼ãƒˆ](https://arxiv.org/abs/2403.13187) | ğŸ“ [ãƒ–ãƒ­ã‚°](https://sakana.ai/evolutionary-model-merge-jp/) | ğŸ¦ [Twitter](https://twitter.com/SakanaAILabs)\n",
        "\n",
        "[EvoVLM-JP](https://huggingface.co/SakanaAI/EvoVLM-v1-JP-7B)ã¯[Sakana AI](https://sakana.ai/)ãŒé–‹ç™ºã—ãŸæ—¥æœ¬èªã«ç‰¹åŒ–ã—ãŸç”»åƒè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚å…¥åŠ›ã—ãŸç”»åƒã«å¯¾ã—ã¦ã€è³ªç–‘å¿œç­”ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚ã‚ˆã‚Šè©³ã—ãã¯ã€ä¸Šè¨˜ã®æŠ€è¡“ãƒ¬ãƒãƒ¼ãƒˆã¨ãƒ–ãƒ­ã‚°ã‚’ã”å‚ç…§ãã ã•ã„ã€‚\n",
        "\"\"\"\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(DESCRIPTION)\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            input_image = gr.Image(type=\"pil\", label=\"å…¥åŠ›ç”»åƒ\")\n",
        "            prompt = gr.Textbox(label=\"è³ªå•\", value=\"ã“ã®å†™çœŸã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\", placeholder=\"å…¥åŠ›ç”»åƒã«é–¢ã™ã‚‹è³ªå•æ–‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\")\n",
        "            # button\n",
        "            input_button = gr.Button(value=\"Submit\")\n",
        "\n",
        "        with gr.Column():\n",
        "            output = gr.Textbox(label=\"ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®è¿”ç­”\")\n",
        "    inputs = [\n",
        "        input_image,\n",
        "        prompt,\n",
        "    ]\n",
        "    input_button.click(fn=inference_fn, inputs=inputs, outputs=[output])\n",
        "    prompt.submit(fn=inference_fn, inputs=inputs, outputs=[output])\n",
        "    img2txt_examples = gr.Examples(\n",
        "        examples=[\n",
        "            [\n",
        "                \"https://images.unsplash.com/photo-1694831404826-3400c48c188d?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\",\n",
        "                \"ã“ã®ä¿¡å·æ©Ÿã®ç‚¹æ»…ã—ã¦ã„ã‚‹è‰²ã¯ä½•è‰²ã§ã™ã‹?\"\n",
        "            ],\n",
        "            [\n",
        "                \"https://user0514.cdnw.net/shared/img/thumb/TSU_mayomayookonomi_TP_V.jpg\",\n",
        "                \"ã“ã®é£Ÿã¹ç‰©ã¯ä½•ã§ã™ã‹ï¼Ÿä½•ã§ã§ãã¦ã„ã¾ã™ã‹ï¼Ÿ\"\n",
        "            ],\n",
        "            [\n",
        "                # \"https://moon.publicdomainq.net/201809/01o/publicdomainq-0025850pkdvxc.jpg\",\n",
        "                \"https://gist.github.com/assets/33302880/a9354891-171e-4925-9216-f5daa40e84e5\",\n",
        "                \"ã“ã®å»ºç‰©ã¯ä½•å¹´èª°ã«ã‚ˆã£ã¦å»ºã¦ã‚‰ã‚Œã¾ã—ãŸã‹ï¼Ÿ\",\n",
        "            ],\n",
        "        ],\n",
        "        fn=inference_fn,\n",
        "        inputs=inputs,\n",
        "        outputs=[output],\n",
        "    )\n",
        "    gr.Markdown(\"\"\"âš ï¸ æœ¬ãƒ¢ãƒ‡ãƒ«ã¯å®Ÿé¨“æ®µéšã®ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã§ã‚ã‚Šã€ç ”ç©¶é–‹ç™ºã®ç›®çš„ã§ã®ã¿æä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚å•†ç”¨åˆ©ç”¨ã‚„ã€éšœå®³ãŒé‡å¤§ãªå½±éŸ¿ã‚’åŠã¼ã™å¯èƒ½æ€§ã®ã‚ã‚‹ç’°å¢ƒï¼ˆãƒŸãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªç’°å¢ƒï¼‰ã§ã®ä½¿ç”¨ã«ã¯é©ã—ã¦ã„ã¾ã›ã‚“ã€‚\n",
        "                æœ¬ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã¯ã€åˆ©ç”¨è€…ã®è‡ªå·±è²¬ä»»ã§è¡Œã‚ã‚Œã€ãã®æ€§èƒ½ã‚„çµæœã«ã¤ã„ã¦ã¯ä½•ã‚‰ä¿è¨¼ã•ã‚Œã¾ã›ã‚“ã€‚\n",
        "                Sakana AIã¯ã€æœ¬ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã«ã‚ˆã£ã¦ç”Ÿã˜ãŸç›´æ¥çš„ã¾ãŸã¯é–“æ¥çš„ãªæå¤±ã«å¯¾ã—ã¦ã€çµæœã«é–¢ã‚ã‚‰ãšã€ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚\n",
        "                åˆ©ç”¨è€…ã¯ã€æœ¬ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã«ä¼´ã†ãƒªã‚¹ã‚¯ã‚’ååˆ†ã«ç†è§£ã—ã€è‡ªèº«ã®åˆ¤æ–­ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒå¿…è¦ã§ã™ã€‚               \n",
        "                ã¾ãŸã€ã“ã®ãƒ‡ãƒ¢ã§ã¯ã€ã§ãã‚‹é™ã‚Šå¤šãã®çš†æ§˜ã«ãŠä½¿ã„ã„ãŸã ã‘ã‚‹ã‚ˆã†ã«ã€å‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’40å˜èªã»ã©ã«åˆ¶é™ã—ã¦ãŠã‚Šã¾ã™ã€‚\"\"\")\n",
        "demo.queue().launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
